{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-25T09:42:05.894093Z",
     "start_time": "2025-12-25T09:39:03.837395Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "try:\n",
    "    import textstat\n",
    "except ImportError:\n",
    "    print(\"Warning: 'textstat' library not found. Please install using 'pip install textstat'\")\n",
    "    textstat = None\n",
    "\n",
    "print(\"Downloading NLTK data...\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "def calculate_readability(text):\n",
    "    \"\"\"\n",
    "    Final Step: Check readability score using Flesch-Kincaid Grade Level.\n",
    "    We apply this on the RAW text (before removing punctuation)\n",
    "    to ensure the sentence count is accurate.\n",
    "    \"\"\"\n",
    "    if textstat:\n",
    "        return textstat.flesch_kincaid_grade(text)\n",
    "    return 0.0\n",
    "\n",
    "def nlp_pipeline(text):\n",
    "    \"\"\"\n",
    "    Complete NLP Pipeline following Slide Page 42 & 50:\n",
    "    1. Basic Cleaning (HTML)\n",
    "    2. Sentence Tokenization\n",
    "    3. Remove Special Chars (Regex)\n",
    "    4. Word Tokenization\n",
    "    5. Lowercasing\n",
    "    6. Stop word removal\n",
    "    7. Lemmatization\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    processed_tokens = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_clean = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "        words = word_tokenize(sentence_clean)\n",
    "\n",
    "        for word in words:\n",
    "            lower_word = word.lower()\n",
    "\n",
    "            if lower_word not in stop_words:\n",
    "                lemma_word = lemmatizer.lemmatize(lower_word)\n",
    "\n",
    "                if len(lemma_word) > 0:\n",
    "                    processed_tokens.append(lemma_word)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STARTING WORKSHOP PIPELINE\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(\"Loading FULL dataset...\")\n",
    "    try:\n",
    "        df = pd.read_csv('IMDB Dataset.csv')\n",
    "        print(f\"Data loaded successfully: {len(df)} rows found.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File 'IMDB Dataset.csv' not found. Please check the file path.\")\n",
    "        return\n",
    "\n",
    "    print(\"Processing...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    if textstat:\n",
    "        print(\"Calculating Readability Scores (Flesch-Kincaid)...\")\n",
    "        df['readability_score'] = df['review'].apply(calculate_readability)\n",
    "        avg_grade = df['readability_score'].mean()\n",
    "    else:\n",
    "        avg_grade = \"N/A (Library not installed)\"\n",
    "\n",
    "    print(\"Running NLP Pipeline (Cleaning -> Tokenizing -> Lemmatizing)...\")\n",
    "    df['processed_tokens'] = df['review'].apply(nlp_pipeline)\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WORKSHOP REPORT (Week 4)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\n[1] Average Flesch-Kincaid Grade Level: {avg_grade}\")\n",
    "    print(\"    (Note: Calculated on original text. Normal range for general text is 8-12)\")\n",
    "\n",
    "    print(\"\\n[2] Regex Explanation (Step 1)\")\n",
    "    print(\"    Regex Used: r'[^a-zA-Z\\\\s]'\")\n",
    "    print(\"    Explanation:\")\n",
    "    print(\"      1. [ ... ] : Defines a Character Set.\")\n",
    "    print(\"      2. ^       : Inside brackets, it means 'Negation' (NOT).\")\n",
    "    print(\"      3. a-zA-Z  : Matches all English alphabets (lowercase and uppercase).\")\n",
    "    print(\"      4. \\\\s      : Matches whitespace characters (spaces, tabs, newlines).\")\n",
    "    print(\"    Logic Translation:\")\n",
    "    print(\"      \\\"Find any single character that is NOT an alphabet AND NOT a space,\")\n",
    "    print(\"       then replace it with an empty string (delete it).\\\"\")\n",
    "\n",
    "    print(\"\\n[3] Sample Comparison (First Row)\")\n",
    "    print(f\"    Original Text : {df['review'].iloc[0][:80]}...\")\n",
    "    print(f\"    Final Tokens  : {df['processed_tokens'].iloc[0][:15]}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Total Processing Runtime: {runtime:.2f} seconds ({runtime/60:.2f} minutes)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Nitro\n",
      "[nltk_data]     V15\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Nitro\n",
      "[nltk_data]     V15\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Nitro\n",
      "[nltk_data]     V15\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Nitro\n",
      "[nltk_data]     V15\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Nitro\n",
      "[nltk_data]     V15\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STARTING WORKSHOP PIPELINE\n",
      "==================================================\n",
      "Loading FULL dataset...\n",
      "Data loaded successfully: 50000 rows found.\n",
      "Processing...\n",
      "Calculating Readability Scores (Flesch-Kincaid)...\n",
      "Running NLP Pipeline (Cleaning -> Tokenizing -> Lemmatizing)...\n",
      "\n",
      "============================================================\n",
      "WORKSHOP REPORT (Week 4)\n",
      "============================================================\n",
      "\n",
      "[1] Average Flesch-Kincaid Grade Level: 9.015685525721471\n",
      "    (Note: Calculated on original text. Normal range for general text is 8-12)\n",
      "\n",
      "[2] Regex Explanation (Step 1)\n",
      "    Regex Used: r'[^a-zA-Z\\s]'\n",
      "    Explanation:\n",
      "      1. [ ... ] : Defines a Character Set.\n",
      "      2. ^       : Inside brackets, it means 'Negation' (NOT).\n",
      "      3. a-zA-Z  : Matches all English alphabets (lowercase and uppercase).\n",
      "      4. \\s      : Matches whitespace characters (spaces, tabs, newlines).\n",
      "    Logic Translation:\n",
      "      \"Find any single character that is NOT an alphabet AND NOT a space,\n",
      "       then replace it with an empty string (delete it).\"\n",
      "\n",
      "[3] Sample Comparison (First Row)\n",
      "    Original Text : One of the other reviewers has mentioned that after watching just 1 Oz episode y...\n",
      "    Final Tokens  : ['one', 'reviewer', 'mentioned', 'watching', 'oz', 'episode', 'youll', 'hooked', 'right', 'exactly', 'happened', 'first', 'thing', 'struck', 'oz']\n",
      "\n",
      "============================================================\n",
      "Total Processing Runtime: 180.78 seconds (3.01 minutes)\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 46
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
